# Week2 Notes

## Lecture9: Review of Lecture8 Content

The Practical Part is neglected.

Video 33:20 for linguistic data source.

adaptive shortcut connections.

The simple yet significant difference between normal RNN and LSTM+GRU is that instead of simply applying the multiplication using the same matrix, there is an "addition" step which ensures the direct linear relation between cell content at $t$ and cell content at $t-1$.

Computing all possible words' distribution is expensive. Possible approaches:

- hierarchical softmax;
- noise-contrastive estimation -- negative sample;
- train on a subset of the vocabulary at a time, test on a smart on the set of possible translations;
- attention to work out what you are translating;
- more: word pieces, char.models

Evaluation metric for MT:

- manual: adeqacy and fluency + error categorization + comparative ranking of translations;
- testing in an application that makes use of MT;
- automatic metric: BLEU, TER, METEOR, ...

BLEU4 formula: counts n-grams up to length4 and take weighted average, removing brevity penalty

overfit is not necessarily evil. Split dataset into train + dev + test.

Build up a complex model slowly. Start with a small subset and increase when the model looks good.

## Lecture10: (Textual) Question Answering

### Motivation/History

Input: a question; Output: the answer to this question

Returning relevant document to a question is of limited use with huge data size nowadays. We can factor this into two parts:

1. finding documents that (might) contain an answer (traditional information retrieval)
2. finding an answer in a paragraph or document (reading comprehension)

P(Passage) + Q(Question) -> A(Answer)

LCC QA system worked fairly well with *factoid question*.

### SQuAD: Standard Question Answering Dataset

Answer must be a span in the passage -- extractive question answering.

Answers may be generated by three real humans and this allows robustness to span choice.

Metrics: exact match + F1 score (harmonic mean of precision and recall)

F1 is preferred as it is less based on choosing exactly the same span that humans chose.

Problem: default SQuAD assumes there exists answer -- SQuAD 2.0: 1/3 have no answer; have a threshold score to determine whethe the span answers the question or introduce another component of "answer validation".

Limitations of SQuAD:

- only span-based answers ( no yes/no, counting, implicit why);
- questions were constructed looking at the passages -- no genuine information needed + generally greater lexical and syntactic match;
- little multi-fact/sentence inference beyond coreherence

But SQuAD 2.0 is still useful and somewhat the best we have right now.

attention score between each word and the bidirectional LSTM hidden state vector from the question to predict start token and end token.

## Lectture11: Convolutional Networks for NLP

Recurrent neural nets cannot capture phrases without prefix context. often have heavy emphasis on last word.

Convolution: usually used to extract features from images. Only 1-dimension convolution is used in this course.

*filter*: a weighted matrix constantly applied to a subsentence to form a shrunk sentence.

*padding*: include zero vector for start and end of the sentence.

*channel*: each filter applied individually

*max pool*: find and extract the maximum value in each channel. -- is there somewhere in the sentence activated regarding the meaning of the channel activated?

*average pool*: find relevance of the sentence to the meaning of the channel.

max pool is usually preferred as the meaning tends to be sparse in natural languages.

*stride*: step size when mooving the filter

*local pooling*: max pooling with vision. max pool within a subrange (length of subrange = stride in local pooling)

*k-max pooling*, *dilated convolution*

batch normalization: transform the convolution output of a batch by scaling the output to be Z.

*1-convolution*: kernel size=1 -- fewer parameters than fully connected network

one application: use CNN as encoder in place of RNN in models similar to seq2seq. Faster in computation as computing being parallelizable.

## Lecture12: Subword Models

morphology little studied in DL, deemed as the minimial level of meaning. character level n-grams may be a better alternative than that morphology alone.

- rich morphology
- transliteration
- informal spelling (esp in social networks)

Two approaches for character-level embeddings

1. word embeddings can be composed from character embeddings -- generate embeddings for unknown words
2. connected language can be processed as characters

human written languages are significantly different -- (fossilized) phonemic + syllabic + ideographic + combination

BPE: Byte Pair Encoding -- originally a *compression* algorithm: most frequent byte pair -> a new byte

start with unigram vocaublary; find most frequent gram pairs -> a new ngram. Automatically decides vocab for the system

Hybrid NMT: translate mostly at word level and only go to the character level when needed.
