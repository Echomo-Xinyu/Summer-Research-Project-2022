# Week2 Notes

## Lecture9: Review of Lecture8 Content

The Practical Part is neglected.

Video 33:20 for linguistic data source.

adaptive shortcut connections.

The simple yet significant difference between normal RNN and LSTM+GRU is that instead of simply applying the multiplication using the same matrix, there is an "addition" step which ensures the direct linear relation between cell content at $t$ and cell content at $t-1$.

Computing all possible words' distribution is expensive. Possible approaches:

- hierarchical softmax;
- noise-contrastive estimation -- negative sample;
- train on a subset of the vocabulary at a time, test on a smart on the set of possible translations;
- attention to work out what you are translating;
- more: word pieces, char.models

Evaluation metric for MT:

- manual: adeqacy and fluency + error categorization + comparative ranking of translations;
- testing in an application that makes use of MT;
- automatic metric: BLEU, TER, METEOR, ...

BLEU4 formula: counts n-grams up to length4 and take weighted average, removing brevity penalty

overfit is not necessarily evil. Split dataset into train + dev + test.

Build up a complex model slowly. Start with a small subset and increase when the model looks good.

## Lecture10: (Textual) Question Answering

### Motivation/History

Input: a question; Output: the answer to this question

Returning relevant document to a question is of limited use with huge data size nowadays. We can factor this into two parts:

1. finding documents that (might) contain an answer (traditional information retrieval)
2. finding an answer in a paragraph or document (reading comprehension)

P(Passage) + Q(Question) -> A(Answer)

LCC QA system worked fairly well with *factoid question*.

### SQuAD: Standard Question Answering Dataset

Answer must be a span in the passage -- extractive question answering.

Answers may be generated by three real humans and this allows robustness to span choice.

Metrics: exact match + F1 score (harmonic mean of precision and recall)

F1 is preferred as it is less based on choosing exactly the same span that humans chose.

Problem: default SQuAD assumes there exists answer -- SQuAD 2.0: 1/3 have no answer; have a threshold score to determine whethe the span answers the question or introduce another component of "answer validation".

Limitations of SQuAD:

- only span-based answers ( no yes/no, counting, implicit why);
- questions were constructed looking at the passages -- no genuine information needed + generally greater lexical and syntactic match;
- little multi-fact/sentence inference beyond coreherence

But SQuAD 2.0 is still useful and somewhat the best we have right now.

attention score between each word and the bidirectional LSTM hidden state vector from the question to predict start token and end token.

## Lectture11: Convolutional Networks for NLP

Recurrent neural nets cannot capture phrases without prefix context. often have heavy emphasis on last word.

Convolution: usually used to extract features from images. Only 1-dimension convolution is used in this course.

*filter*: a weighted matrix constantly applied to a subsentence to form a shrunk sentence.

*padding*: include zero vector for start and end of the sentence.

*channel*: each filter applied individually

*max pool*: find and extract the maximum value in each channel. -- is there somewhere in the sentence activated regarding the meaning of the channel activated?

*average pool*: find relevance of the sentence to the meaning of the channel.

max pool is usually preferred as the meaning tends to be sparse in natural languages.

*stride*: step size when mooving the filter

*local pooling*: max pooling with vision. max pool within a subrange (length of subrange = stride in local pooling)

*k-max pooling*, *dilated convolution*

batch normalization: transform the convolution output of a batch by scaling the output to be Z.

*1-convolution*: kernel size=1 -- fewer parameters than fully connected network

one application: use CNN as encoder in place of RNN in models similar to seq2seq. Faster in computation as computing being parallelizable.

## Lecture12: Subword Models

morphology little studied in DL, deemed as the minimial level of meaning. character level n-grams may be a better alternative than that morphology alone.

- rich morphology
- transliteration
- informal spelling (esp in social networks)

Two approaches for character-level embeddings

1. word embeddings can be composed from character embeddings -- generate embeddings for unknown words
2. connected language can be processed as characters

human written languages are significantly different -- (fossilized) phonemic + syllabic + ideographic + combination

BPE: Byte Pair Encoding -- originally a *compression* algorithm: most frequent byte pair -> a new byte

start with unigram vocaublary; find most frequent gram pairs -> a new ngram. Automatically decides vocab for the system

Hybrid NMT: translate mostly at word level and only go to the character level when needed.

## Lecture13: Contextual Word Embeddings and Pretraining

A random tip for QA system building: when encountering an unknown word at test time, we can consider assigning a random vector and compare its output against it -- the output is likely to be similar to the random vector assigned and this will allow us to use the "unknown" word directly in answer.

Representations for a word: word2vec / gloVE, fastText.

Problem of existing representation:

- same representation for a word regardless of the context where a word token occurs -- we want word sense disambiguation;
- only one representation for a word, but words have different aspects, including semantics, syntactic behavior, and register/connotations;

### ELMo

Hidden states at LSTM are meanted to consider the context and words' representations at corresponding position.

The above approach can be used as a layer to extract better embeddings with context for further learning.

ELMo: Embeddings from Language Models: breakout version from traditional word embeddings to contextual word vectors.

ELMo use all layers (with weights) instead of merely the top layer. -- ~~how to achieve that remains somewhat mysterious~~.

Different layers in ELMo tend to have different strength in different areas. (Areas to be explored)

**ULMfit**: transfer learning from a general model to a specific task. Limit size to be small -- larger version by others include GPT.

### Transformer Intro

Unable to parallelize works in RNN. RNNs still need attention mechanism to deal with long range dependencies - path length. We can use attention alone then.

**Multi-head attention**: map hidden state into different lower dimension spaces and compute attentions separately.

No free lunch: trade off between long chained information and performance

Add positional information into encoder.

### BERT: BiDirectional encoder Representations from Transformers

LMs only use unidirectional context -- we can improve on that. -- words can be seen themselves in bidirectional context.

Mask out certain input words and predict the masked words.

Next Sentence prediction: learn relationships between sentences and to predict whether sentence A is the next sentence of sentence B.

Token embeddings are word pieces. Learned segmented embedding represents each sentence and positional embedding is as for other transformer architectures.

Multi-sentence Natural Language Inference: determine relations between hypothesis and premise.

## Lecture14: Transformers and Self-Attention

Attention -> self attention -> multi-head attention

tensor2tensor + sockeye + etc

Residual: carry positional information to higher laters, among other information.

*The whole lecture content appears to be rather superficial to me. It is fairly hard to grasp the concept without really understanding the concepts. So I will rewatch this lecture and update this section of notes again when I feel more confident in this area.*
